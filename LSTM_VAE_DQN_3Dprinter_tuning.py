# ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    f1_score, 
    precision_score,
    recall_score,
    accuracy_score
)
from gymnasium import Env
from gymnasium.spaces import Box, Discrete
from stable_baselines3 import DQN
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.logger import configure
import matplotlib.pyplot as plt

# ë°ì´í„° ê²½ë¡œ ì„¤ì •
base_path = r"C:\Users\User\Desktop\dongmin\3d_printer_vib_rms_normal"
selected_indices = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
file_names = [f"3d_printer_vib_rms_normal_{i}.csv" for i in selected_indices]

# ëª¨ë“  RMS ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
all_rms = []
for file in file_names:
    file_path = os.path.join(base_path, file)
    df = pd.read_csv(file_path, encoding='utf-8')
    all_rms.append(df['RMS'].values)

# ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸° (ê°€ì¥ ì§§ì€ ê¸¸ì´ì— ë§ì¶¤)
min_len = min(len(rms) for rms in all_rms)
all_rms_aligned = [rms[:min_len] for rms in all_rms]

# ë°ì´í„° ì •ê·œí™” (ê°œë³„ íŒŒì¼ë³„ë¡œ ì •ê·œí™”)
scaler = MinMaxScaler()
all_rms_scaled = []
for rms in all_rms_aligned:
    scaled = scaler.fit_transform(rms.reshape(-1, 1)).flatten()
    all_rms_scaled.append(scaled)
all_rms_array = np.array(all_rms_scaled)  # shape: (30, min_len)

# ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_sequences(data, seq_length):
    sequences = []
    for rms in data:
        for i in range(len(rms) - seq_length + 1):
            seq = rms[i:i+seq_length]
            sequences.append(seq.reshape(-1, 1))  # (seq_length, 1)
    return np.array(sequences)

SEQ_LENGTH = 30
X = create_sequences(all_rms_array, SEQ_LENGTH)
X_train_full, X_test = train_test_split(X, test_size=0.2, shuffle=False)
# DQNì„ ìœ„í•œ validation set ë¶„ë¦¬
X_train, X_val = train_test_split(X_train_full, test_size=0.2, shuffle=False)

# ë…¸ì´ì¦ˆ ì¶”ê°€ í•¨ìˆ˜ ì •ì˜
def add_noise(data, noise_factor=0.05):
    noise = np.random.normal(loc=0, scale=noise_factor, size=data.shape)
    noisy_data = data + noise
    return np.clip(noisy_data, 0.0, 1.0)

# ë…¸ì´ì¦ˆ ì¶”ê°€ëœ í•™ìŠµ ë°ì´í„° ìƒì„±
X_train_noisy = add_noise(X_train)
X_val_noisy = add_noise(X_val)

# LSTM-VAE ëª¨ë¸ ì •ì˜
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        epsilon = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

# Encoder (ì…ë ¥ shape: (SEQ_LENGTH, 1))
encoder_inputs = layers.Input(shape=(SEQ_LENGTH, 1))
x = layers.LSTM(64, return_sequences=True)(encoder_inputs)
x = layers.LSTM(32)(x)
z_mean = layers.Dense(8)(x)
z_log_var = layers.Dense(8)(x)
z = Sampling()([z_mean, z_log_var])
encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name="encoder")

# Decoder
latent_inputs = layers.Input(shape=(8,))
x = layers.RepeatVector(SEQ_LENGTH)(latent_inputs)
x = layers.LSTM(32, return_sequences=True)(x)
x = layers.LSTM(64, return_sequences=True)(x)
decoder_outputs = layers.TimeDistributed(layers.Dense(1))(x)
decoder = Model(latent_inputs, decoder_outputs, name="decoder")

# VAE êµ¬ì„±
vae_outputs = decoder(encoder(encoder_inputs)[2])
vae = Model(encoder_inputs, vae_outputs, name="vae")

vae.encoder = encoder
vae.decoder = decoder

# Loss í•¨ìˆ˜
reconstruction_loss = tf.reduce_mean(
    tf.reduce_sum(tf.square(encoder_inputs - vae_outputs), axis=[1,2])
)
kl_loss = -0.5 * tf.reduce_mean(
    tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)
)
vae_loss = reconstruction_loss + kl_loss
vae.add_loss(vae_loss)

# í•™ìŠµ ê³¡ì„  ì €ì¥ìš© Callback
class LossHistory(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs=None):
        self.losses = []
        self.val_losses = []

    def on_epoch_end(self, epoch, logs=None):
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))

# ë°°ì¹˜ ë‹¨ìœ„ ì†ì‹¤ ê¸°ë¡
class BatchLossHistory(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs=None):
        self.batch_losses = []

    def on_train_batch_end(self, batch, logs=None):
        self.batch_losses.append(logs.get('loss'))

batch_loss_history = BatchLossHistory()
loss_history = LossHistory()

# VAE ì»´íŒŒì¼ ë° í•™ìŠµ
vae.compile(optimizer='adam')
vae.fit(
    X_train_noisy, X_train,
    epochs=30,
    batch_size=64,
    validation_data=(X_val_noisy, X_val),
    callbacks=[loss_history, batch_loss_history, tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]
)

# í•™ìŠµ ê³¡ì„  ì‹œê°í™”
#plt.figure(figsize=(12, 4))
#plt.subplot(1, 2, 1)
#plt.plot(loss_history.losses, label='Train Loss')
#plt.plot(loss_history.val_losses, label='Validation Loss')
#plt.title('Epoch Loss')
#plt.legend()

#plt.subplot(1, 2, 2)
#plt.plot(batch_loss_history.batch_losses)
#plt.title('Batch Loss')
#plt.tight_layout()
#plt.show()


# ì´ìƒ ë°ì´í„° ì£¼ì…
def inject_anomalies(data, ratio=0.07, min_strength=3.0, max_strength=6.0):
    data = data.copy()
    n_anomalies = int(len(data) * ratio)
    anomaly_indices = []
    seq_len = data.shape[1]
    curve_types = ['spike'] * 6 + ['step', 'linear', 'exp', 'gradual_increase']
    used_indices = set()
    while len(anomaly_indices) < n_anomalies:
        idx = np.random.randint(0, len(data))
        # ì¸ì ‘ ì´ìƒì¹˜ ë°©ì§€ (í•œ ì‹œí€€ìŠ¤ì— ì´ìƒì¹˜ê°€ ëª°ë¦¬ì§€ ì•Šê²Œ)
        if any(abs(idx - uidx) < 5 for uidx in used_indices):
            continue
        used_indices.add(idx)
        anomaly_indices.append(idx)
        strength = np.random.uniform(min_strength, max_strength)
        drift_magnitude = strength * np.std(data)
        curve_type = np.random.choice(curve_types)
        if curve_type == 'spike':
            spike_start = np.random.randint(seq_len//4, 3*seq_len//4)
            spike_width = np.random.randint(1, 3)
            drift = np.zeros(seq_len)
            drift[spike_start:spike_start+spike_width] = drift_magnitude * np.random.uniform(3.0, 5.0)
        elif curve_type == 'step':
            step_point = np.random.randint(seq_len//4, 3*seq_len//4)
            drift = np.concatenate([
                np.zeros(step_point),
                np.full(seq_len-step_point, drift_magnitude)
            ])
        elif curve_type == 'linear':
            drift = np.linspace(0, drift_magnitude, seq_len)
        elif curve_type == 'exp':
            x = np.linspace(0, 1, seq_len)
            drift = drift_magnitude * (np.exp(3*x) - 1) / (np.exp(3) - 1)
        elif curve_type == 'gradual_increase':
            start_point = np.random.randint(seq_len//3)
            drift = np.zeros(seq_len)
            drift[start_point:] = np.linspace(0, drift_magnitude, seq_len-start_point)
        data[idx] += drift.reshape(-1, 1)
    return data, anomaly_indices

X_val_anomalous, val_anomaly_idx = inject_anomalies(X_val)
X_test_anomalous, test_anomaly_idx = inject_anomalies(X_test)

# ê°•í™”í•™ìŠµ í™˜ê²½ ì •ì˜
class DynamicThresholdEnv(Env):
    def __init__(self, model, data, anomaly_indices):
        super().__init__()
        self.model = model
        self.data = data
        self.anomaly_indices = anomaly_indices
        self.current_step = 0
        self.action_space = Discrete(41) # 0~40
        self.observation_space = Box(low=0.0, high=1.0, shape=(23,), dtype=np.float32)
        self.reset()

    def reset(self, seed=None, options=None):
        self.current_step = 0
        self.recent_errors = [0.0, 0.0, 0.0]
        self.recent_pred_errors = []
        self.current_threshold = 0.15
        self.prev_threshold = 0.15
        self.fn_streak = 0
        self.fp_streak = 0
        self.recent_predictions = []
        self.error_history = []
        self.tn_count = 0
        self.tp_count = 0
        self.fp_count = 0
        self.fn_count = 0
        return self._get_obs(), {}

    def _get_obs(self):
        window = self.data[self.current_step]
        z_mean, _, _ = self.model.encoder.predict(window[np.newaxis], verbose=0)
        recon = self.model.predict(window[np.newaxis], verbose=0).squeeze(0)
        error = np.mean(np.abs(window - recon))
        error_vec = np.mean(np.abs(window - recon), axis=0)
        error = np.clip(error, 0, 1)
        error_vec = np.clip(error_vec, 0, 1)

        self.error_history.append(error)
        if len(self.error_history) > 10:
            self.error_history.pop(0)

        error_mean = np.mean(self.error_history)
        error_std = np.std(self.error_history)
        error_trend = self.error_history[-1] - self.error_history[0] if len(self.error_history) > 1 else 0

        self.recent_errors = self.recent_errors[1:] + [error]
        error_diff = self.recent_errors[-1] - self.recent_errors[-2]
        error_diff = np.clip(error_diff, -1, 1) * 0.5 + 0.5

        min_err = np.min(self.error_history)
        max_err = np.max(self.error_history)
        median_err = np.median(self.error_history)
        last3 = self.error_history[-3:] if len(self.error_history) >= 3 else [0,0,0]

        # ìµœê·¼ FP/FN ë¹„ìœ¨
        if len(self.recent_predictions) > 0:
            recent_fp_rate = sum(1 for p in self.recent_predictions[-10:] if p == 'FP') / min(10, len(self.recent_predictions))
            recent_fn_rate = sum(1 for p in self.recent_predictions[-10:] if p == 'FN') / min(10, len(self.recent_predictions))
        else:
            recent_fp_rate = 0.0
            recent_fn_rate = 0.0

        obs = np.concatenate([
            z_mean[0].flatten()[:8],
            np.array([
                self.recent_errors[-2], self.recent_errors[-1], error_diff,
                error_mean, error_std, error_trend,
                min_err, max_err, median_err,
                *last3,
                self.current_threshold / 0.5,
                recent_fp_rate, recent_fn_rate
            ])
        ])

        return obs.astype(np.float32)

    def step(self, action):
        self.current_step += 1
        done = self.current_step >= len(self.data) - 1

        self.current_threshold = 0.02 + action * 0.003
        self.current_threshold = np.clip(self.current_threshold, 0.02, 0.15)

        window = self.data[self.current_step]
        recon = self.model.predict(window[np.newaxis], verbose=0)
        error = np.mean(np.abs(window - recon))

        is_anomaly = self.current_step in self.anomaly_indices
        y_true = 1 if is_anomaly else 0
        y_pred = 1 if error > self.current_threshold else 0

        # ì¶•ì†Œëœ ë³´ìƒ êµ¬ì¡°
        if y_true == 1 and y_pred == 1:  # TP
            reward = 1.0
            self.tp_count += 1
            self.fn_streak = 0
            self.fp_streak = 0
            prediction_type = 'TP'
        elif y_true == 1 and y_pred == 0:  # FN
            reward = -0.8
            self.fn_count += 1
            self.fn_streak += 1
            self.fp_streak = 0
            prediction_type = 'FN'
        elif y_true == 0 and y_pred == 1:  # FP
            reward = -0.3
            self.fp_count += 1
            self.fp_streak += 1
            self.fn_streak = 0
            prediction_type = 'FP'
        else:  # TN
            reward = 0.2
            self.tn_count += 1
            self.fn_streak = 0
            self.fp_streak = 0
            prediction_type = 'TN'

        self.recent_predictions.append(prediction_type)
        if len(self.recent_predictions) > 20:
            self.recent_predictions.pop(0)

        # ì—°ì† FN íŒ¨ë„í‹° (ì¶•ì†Œ)
        if self.fn_streak > 2:
            reward -= 0.05 * self.fn_streak

        # ì—°ì† FP íŒ¨ë„í‹° (ì¶•ì†Œ)
        if self.fp_streak > 3:
            reward -= 0.05 * (self.fp_streak - 3)

        # Precision ë‚®ì„ ë•Œ íŒ¨ë„í‹°
        recent_precision = self.tp_count / (self.tp_count + self.fp_count) if (self.tp_count + self.fp_count) > 0 else 1.0
        if recent_precision < 0.6:
            reward -= 0.1

        # threshold ë³€í™” ë¹„ìš© (ì¶•ì†Œ)
        threshold_change_cost = abs(self.current_threshold - self.prev_threshold) * 0.02
        reward -= threshold_change_cost
        self.prev_threshold = self.current_threshold

        # ê·¹ë‹¨ì  threshold í˜ë„í‹° (ìœ ì§€)
        if self.current_threshold >= 0.30:
            reward -= 0.1
        elif self.current_threshold <= 0.05:
            reward -= 0.05

        if self.current_step % 100 == 0 or done:
            print(f"[Step {self.current_step}] TP={self.tp_count} FN={self.fn_count} FP={self.fp_count} TN={self.tn_count}")
            print(f"  Precision={recent_precision:.3f} FP_streak={self.fp_streak} FN_streak={self.fn_streak}")
            print(f"  Threshold={self.current_threshold:.3f} Reward={reward:.2f} Error={error:.4f}")

        return self._get_obs(), reward, done, False, {}

# ì¶”ë¡  í•¨ìˆ˜
def dynamic_threshold_detection_fast(model, data):
    z_mean, _, _ = vae.encoder.predict(data, verbose=0)
    recon = vae.predict(data, verbose=0)
    errors = np.mean(np.abs(data - recon), axis=(1, 2))
    thresholds = []
    error_history = []
    
    for i in range(len(data)):
        error_history.append(errors[i])
        if len(error_history) > 10:
            error_history.pop(0)
        
        error_mean = np.mean(error_history)
        error_std = np.std(error_history)
        error_trend = error_history[-1] - error_history[0] if len(error_history) > 1 else 0
        
        if i >= 2:
            recent_errors = [errors[i-2], errors[i-1], errors[i]]
            error_diff = errors[i] - errors[i-1]
        elif i == 1:
            recent_errors = [0.0, errors[i-1], errors[i]]
            error_diff = errors[i] - errors[i-1]
        else:
            recent_errors = [0.0, 0.0, errors[i]]
            error_diff = 0.0
        
        error_diff = np.clip(error_diff, -1, 1) * 0.5 + 0.5
        min_err = np.min(error_history)
        max_err = np.max(error_history)
        median_err = np.median(error_history)
        last3 = error_history[-3:] if len(error_history) >= 3 else [0,0,0]
        recent_fp_rate = 0.0
        recent_fn_rate = 0.0
        current_threshold = thresholds[-1] if thresholds else 0.15
        
        obs = np.concatenate([
            z_mean[i].flatten()[:8],
            np.array([
                recent_errors[-2], recent_errors[-1], error_diff,
                error_mean, error_std, error_trend,
                min_err, max_err, median_err,
                *last3,
                current_threshold / 0.5,
                recent_fp_rate, recent_fn_rate
            ])
        ])
        
        action, _ = model.predict(obs[np.newaxis, :].astype(np.float32), deterministic=True)
        threshold = 0.08 + action[0] * 0.006
        thresholds.append(threshold)
    
    return np.array(thresholds)

# í‰ê°€ í•¨ìˆ˜
def evaluate(model, X, true_anomalies):
    recon = vae.predict(X, verbose=0)
    errors = np.mean(np.abs(X - recon), axis=(1, 2))
    if isinstance(model, DQN):
        thresholds = dynamic_threshold_detection_fast(model, X)
        y_pred = (errors > thresholds).astype(int)
    else:
        threshold = np.percentile(errors, 95)
        y_pred = (errors > threshold).astype(int)
    y_true = np.zeros(len(X), dtype=int)
    y_true[true_anomalies] = 1
    f1 = f1_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    accuracy = accuracy_score(y_true, y_pred)
    print(f"F1 Score : {f1:.4f}")
    print(f"Precision : {precision:.4f}")
    print(f"Recall : {recall:.4f}")
    print(f"Accuracy : {accuracy:.4f}")
    return f1, precision, recall, accuracy

# í™˜ê²½ ì„¤ì •
train_env = Monitor(DynamicThresholdEnv(vae, X_val_anomalous, val_anomaly_idx))
log_path = "./dqn_logs/"
os.makedirs(log_path, exist_ok=True)

# 16ê°œ ì‹¤í—˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
param_grid = [
    {   # ì‹¤í—˜ 11: ê· í˜• ì¡íŒ F1 ìµœì í™” ì„¸íŒ…
        'learning_rate': 3e-4,
        'batch_size': 64,
        'buffer_size': 180000,
        'exploration_fraction': 0.35,
        'exploration_final_eps': 0.08,
        'target_update_interval': 1500,
        'gamma': 0.985,
        'train_freq': 3,
        'gradient_steps': 2,
        'policy_kwargs': dict(net_arch=[192, 96]),
        'tensorboard_log': log_path
    }
]

# ê·¸ë¦¬ë“œ ì„œì¹˜ ìˆ˜í–‰
best_score = -np.inf
best_model = None
best_params = None
results = []

for idx, params in enumerate(param_grid):
    print(f"\n=== Experiment {idx+1}/{len(param_grid)} ===")
    print(params)
    
    try:
        # ìƒˆ ë¡œê±° ìƒì„±
        exp_logger = configure(f"{log_path}/exp_{idx+1}", ["stdout", "csv", "tensorboard"])
        
        # ëª¨ë¸ ìƒì„±
        model = DQN(
            "MlpPolicy",
            train_env,
            verbose=1,
            **{k:v for k,v in params.items() if k != 'tensorboard_log'}
        )
        model.set_logger(exp_logger)
        
        # í•™ìŠµ
        model.learn(total_timesteps=50000)
        
        # ì„±ëŠ¥ í‰ê°€
        f1, precision, recall, accuracy = evaluate(model, X_test_anomalous, test_anomaly_idx)
        score = f1 * 0.5 + precision * 0.3 + recall * 0.2  # ë³µí•© í‰ê°€ ì ìˆ˜
        
        # ê²°ê³¼ ì €ì¥
        results.append({
            'params': params,
            'f1': f1,
            'precision': precision,
            'recall': recall,
            'accuracy': accuracy,
            'score': score
        })
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê°±ì‹ 
        if score > best_score:
            best_score = score
            best_model = model
            best_params = params
            print(f"ğŸ”¥ New best score: {best_score:.4f}")
            
    except Exception as e:
        print(f"âŒ Experiment {idx+1} failed: {e}")
        continue

# ìµœì¢… ê²°ê³¼ ì¶œë ¥
print("\n\n=== Final Results ===")
for i, res in enumerate(sorted(results, key=lambda x: x['score'], reverse=True)):
    print(f"\nRank {i+1} | Score: {res['score']:.4f}")
    print(f"F1: {res['f1']:.4f}, Precision: {res['precision']:.4f}, Recall: {res['recall']:.4f}")
    print("Hyperparameters:")
    for k, v in res['params'].items():
        print(f"  {k}: {v}")